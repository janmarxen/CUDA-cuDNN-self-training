{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e4e3161-8706-4a8f-a0e8-c0dbaad039cb",
   "metadata": {},
   "source": [
    "Massive parallel programming on GPUs and applications, by Lokman ABBAS TURKI  \n",
    "\n",
    "# 3. Add arrays\n",
    "\n",
    "## 3.1 Objective\n",
    "\n",
    "The main purpose of this lab is to familiarize students with the CUDA API through the implementation of vector addition. Students will gain hands-on experience by writing both GPU kernel code and the corresponding host code for array addition. The host code for memory allocation on GPU and copies from CPU to GPU and from GPU to CPU will serve as examples for future labs. We also introduce the use of unified memory.\n",
    "\n",
    "Students are encouraged to use the CUDA documentation, enabling them to discover:\n",
    "\n",
    "1) the specifications of CUDA API functions within the [CUDA_Runtime_API](https://docs.nvidia.com/cuda/cuda-runtime-api/index.html).\n",
    "2) the examples of how to use the CUDA API functions in [CUDA_C_Programming_Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248f1393-e53b-4d0f-9787-502ab785ebf8",
   "metadata": {},
   "source": [
    "## 3.2 Content\n",
    "\n",
    "Your directory has to contain Add.cu file as well as the header file timer.h\n",
    "\n",
    "Compile Add.cu using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6008012e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc Add.cu -o ADD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d674be7",
   "metadata": {},
   "source": [
    "Execute DQ using (on Windows machine ./ is not needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17f78bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ( 1410064908 ): 1410064908\n",
      " ( 1410064918 ): 1410064918\n",
      " ( 1410064928 ): 1410064928\n",
      " ( 1410064938 ): 1410064938\n",
      " ( 1410064948 ): 1410064948\n",
      "CPU Timer for the addition on the CPU of vectors: 5.008873 s\n",
      "Kernel execution time: 0.108118 seconds\n"
     ]
    }
   ],
   "source": [
    "!./ADD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e625b987-e955-4fc4-bd55-fdec28ed8918",
   "metadata": {},
   "source": [
    "As long as you did not include any additional instruction in the file Add.cu, the execution above is supposed to return\n",
    "\n",
    "( 999999500 ): 999999500\n",
    "( 999999510 ): 999999510\n",
    "( 999999520 ): 999999520\n",
    "( 999999530 ): 999999530\n",
    "( 999999540 ): 999999540\n",
    "CPU Timer for the addition on the CPU of vectors: 0.126\n",
    "\n",
    "Of course, the execution time changes at each call and depends on the host's performance.\n",
    "\n",
    "### 3.2.1 Addition operation on the device with explicit data transfer and CPU timer\n",
    "\n",
    "a) Allocate aGPU, bGPU, cGPU on the GPU using cudaMalloc.\n",
    "\n",
    "b) Transfer the values of a, b to aGPU, bGPU using cudaMemcpy.\n",
    "\n",
    "c) Write the kernel addVect_k that adds aGPU to bGPU and puts the result in cGPU\n",
    "\n",
    "d) Transfer the values of cGPU to c using cudaMemcpy.\n",
    "\n",
    "e) Free the GPU memory using cudaFree.\n",
    "\n",
    "f) Call the kernel addVect_k instead of the function addVect.\n",
    "\n",
    "g) Do not forget to use cudaDeviceSynchronize after calling the kernel.\n",
    "\n",
    "\n",
    "### 3.2.2 Few Optimizations and GPU timer\n",
    "\n",
    "a) Change the CPU timer with the GPU timer using cudaEvent (in milliseconds).\n",
    "\n",
    "b) Check for yourself that using threadIdx.x*gridDim.x + blockIdx.x to access the global memory is a very bad choice.\n",
    "\n",
    "c) What if you compute the execution time of both calling addVect_k and transferring data?\n",
    "\n",
    "d) Profile further your code using: !nvprof ./ADD\n",
    "\n",
    "e) Fix the slow transfer of a, b to aGPU, bGPU using the initialization on the device. Now, we can remove the arrays a, and b.\n",
    "\n",
    "f) Allocate aGPU, bGPU, and cGPU using cudaMallocManaged on the unified memory. Now we can also remove the array c.\n",
    "\n",
    "g) What if we kept the initialization on the host but with all arrays in the unified memory?\n",
    "\n",
    "h) You can speed up solution g) using cudaMemPrefetchAsync of a and b before calling addVect_k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe988e92-f520-4aba-95c0-5513032894a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ( 1410064908 ): 1410064908\n",
      " ( 1410064918 ): 1410064918\n",
      " ( 1410064928 ): 1410064928\n",
      " ( 1410064938 ): 1410064938\n",
      " ( 1410064948 ): 1410064948\n",
      "CPU Timer for the addition on the CPU of vectors: 4.998796 s\n",
      "==30737== NVPROF is profiling process 30737, command: ./ADD\n",
      "Kernel execution time: 0.022129 seconds\n",
      "==30737== Profiling application: ./ADD\n",
      "==30737== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      " GPU activities:   76.68%  695.29us         1  695.29us  695.29us  695.29us  initVect_k(int*, int*, int)\n",
      "                   23.32%  211.49us         1  211.49us  211.49us  211.49us  addVect_k(int*, int*, int*, int)\n",
      "      API calls:   83.76%  117.55ms         2  58.774ms     630ns  117.55ms  cudaEventCreate\n",
      "                   14.65%  20.563ms         3  6.8544ms  49.580us  20.451ms  cudaMallocManaged\n",
      "                    0.64%  903.09us         1  903.09us  903.09us  903.09us  cudaDeviceSynchronize\n",
      "                    0.46%  644.85us         2  322.43us  6.5100us  638.34us  cudaLaunchKernel\n",
      "                    0.28%  392.65us         3  130.88us  68.300us  249.05us  cudaFree\n",
      "                    0.16%  226.49us       114  1.9860us      80ns  92.801us  cuDeviceGetAttribute\n",
      "                    0.02%  23.590us         1  23.590us  23.590us  23.590us  cuDeviceGetName\n",
      "                    0.02%  21.981us         2  10.990us  3.9310us  18.050us  cudaEventRecord\n",
      "                    0.01%  10.841us         1  10.841us  10.841us  10.841us  cuDeviceGetPCIBusId\n",
      "                    0.00%  4.9200us         1  4.9200us  4.9200us  4.9200us  cudaEventElapsedTime\n",
      "                    0.00%  2.0400us         2  1.0200us     360ns  1.6800us  cudaEventDestroy\n",
      "                    0.00%  1.9200us         3     640ns     150ns  1.4100us  cuDeviceGetCount\n",
      "                    0.00%     410ns         1     410ns     410ns     410ns  cuModuleGetLoadingMode\n",
      "                    0.00%     360ns         1     360ns     360ns     360ns  cuDeviceTotalMem\n",
      "                    0.00%     320ns         2     160ns      80ns     240ns  cuDeviceGet\n",
      "                    0.00%     160ns         1     160ns     160ns     160ns  cuDeviceGetUuid\n",
      "\n",
      "==30737== Unified Memory profiling result:\n",
      "Device \"Tesla V100-PCIE-16GB (0)\"\n",
      "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
      "       2         -         -         -           -  896.3760us  Gpu page fault groups\n"
     ]
    }
   ],
   "source": [
    "!nvprof ./ADD"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
